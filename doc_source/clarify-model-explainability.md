# Model explainability<a name="clarify-model-explainability"></a>

Amazon SageMaker Clarify provides tools to help us explain the predictions of a deployed machine learning \(ML\) model producing inferences\. These tools can help ML modelers and developers and other internal stakeholders understand model characteristics as a whole prior to deployment and to debug predictions provided by the model once deployed\. Transparency about how ML models arrive at their predictions is also critical to consumers and regulators who need to trust the model predictions if they are going to make decisions based on them\. SageMaker Clarify use a on model\-agnostic feature attribution approaches, which can be used both for global understanding of a model after training and for providing per\-instance explanation during inference\. The implementation includes a scalable and efficient implementation of [SHAP](https://papers.nips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf), based on the concept of a Shapley value from the field of cooperative game theory that assigns each feature an importance value for a particular prediction\.

What is the function of an explanation in the machine learning context? An explanation can be thought of as the answer to a why\-question that helps human understand of the cause of a decision\. Explanations help to create a shared understanding of the decision between a human and an intelligent agent such as a machine learned model\. Humans tend to be interested in explanations especially when they encounter events that are unexpected or violate their existing mental model, and hence surprising to them\. In the context of a machine learning model, we may be interested in answering questions such as “Why did the model predict a negative outcome such as a loan rejection for a given applicant?”, “When I can trust the model? How does the model make predictions?”, and “Why did the model make an incorrect prediction? Which features have the largest influence on the behavior of the model?” Explanations can be useful for auditing and meeting regulatory requirements, building trust in the model and supporting human decision making, and debugging and improving model performance\.

The need to satisfy the demands for human understanding about the nature and outcomes of ML inference is key to the sort of explanation needed\. Research from philosophy and cognitive science disciplines has shown that people care especially about contrastive explanations, or explanations of why an event X happened instead of some other event Y that did not occur\. Here, X could be an unexpected or surprising event that happened and Y corresponds to an expectation based on their existing mental model referred to as a *baseline*\. Note that for the same event X, different people may seek different explanations depending on their point of view or mental model Y\. In the context of explainable AI, we can think of X as the example being explained and Y as a baseline that is typically chosen to represent a uninformative or average example in the dataset\. Sometimes, the baseline may be implicit as with an image with all pixels of the same color in the case of ML models for images\.

**Topics**
+ [Feature Attributions using Shapley Values](clarify-shapley-values.md)
+ [SHAP baselines for explainability](clarify-feature-attribute-shap-baselines.md)
+ [Create feature attribute baselines and explainability reports](clarify-feature-attribute-baselines-reports.md)
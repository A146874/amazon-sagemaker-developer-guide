# What is fairness and model explainability for machine learning predictions?<a name="clarify-fairness-and-explainability"></a>

Amazon SageMaker Clarify helps improve your machine learning models by detecting potential bias and helping explain the predictions that models make\. It helps identify various types of bias in pre\-training data and in post\-training that can emerge during model training or when in production\. SageMaker Clarify helps explain how these models make predictions using a feature attribution approach\. It also monitors inferences models make in production for bias or feature attribution drift\. The fairness and explainability functionality provided by SageMaker Clarify provides components that help AWS customers build less biased and more understandable machine learning models\. It also provides tools to help you generate model governance reports which can be used to inform risk and compliance teams and external regulators\.

Machine learning models and data\-driven systems are being increasingly used to help make decisions across domains such as financial services, healthcare, education, and human resources\. Machine learning applications provide benefits such as improved accuracy, increased productivity, and cost savings\. Explainable AI and tools that help identify potential bias in ML for regulatory requirements, business decisions, and data science procedures\.
+ **Regulatory**: In many situations, it is important to understand why the ML model made a specific prediction and also whether the prediction it made was impacted by any bias, either during training or at inference\. Recently, policymakers, regulators, and advocates have raised awareness about the ethical and policy challenges posed by ML and data\-driven systems\. In particular, they have expressed concerns about the potentially discriminatory impact of such systems, for example, due to inadvertent encoding of bias into automated decisions\. 
+ **Business**: The adoption of AI systems in regulated domains requires trust and this is built by providing reliable explanations of the behavior of trained models and how the deployed models make predictions\. Model explainability may be particularly important to certain industries with reliability, safety, and compliance requirements, such as financial services and human resources, healthcare, and automated transportation\. To take a common financial example, lending applications that incorporate the use of ML models may need to provide explanations about how those models made certain predictions to internal teams of loan officers, customer service representatives, and forecasters, as well as to end users/customers\.
+ **Data Science**: Data scientists and ML engineers need tools to generate the insights required to debug and improve ML models through better feature engineering, to determine whether a model is making inferences based on noisy or irrelevant features, and to understand the limitations of their models and failure modes their models may encounter\.

## Best practices for evaluating fairness and explainability in the ML lifecycle<a name="clarify-fairness-and-explainability-best-practices"></a>

**Fairness as a Process**: The notions of bias and fairness are highly application dependent and the choice of the attribute\(s\) for which bias is to be measured, as well as the choice of bias metrics, should be guided by ethical, business, and regulatory considerations\. Building consensus and achieving collaboration across key stakeholders \(such as product, policy, legal, PR, engineering, and AI/ML teams, as well as end users and communities\) is important for the successful adoption of fair and transparent ML applications\.

**Fairness and Explainability by Design in the ML Lifecycle**: Fairness and explainability should be considered during each stage of the ML lifecycle: Problem Formation, Dataset Construction, Algorithm Selection, Model Training Process, Testing Process, Deployment, and Monitoring/Feedback\. It is important to have the right tools to do this analysis\. To encourage engaging with these considerations, here are a few example questions worth asking during each of these stages\.

![\[Best practices for the process of evaluating fairness and model explainability.png.\]](http://docs.aws.amazon.com/sagemaker/latest/dg/images/clarify-best-practices-image.png)

## Sample notebooks<a name="clarify-fairness-and-explainability-sample-notebooks"></a>

Amazon SageMaker Clarify provides the following sample notebooks\.
+ [Explainability and bias detection with Amazon SageMaker Clarify](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/fairness_and_explainability/fairness_and_explainability.ipynb): Use SageMaker Clarify to create a processing job for the detecting bias and explaining model predictions with feature attributions\.
+ [Monitoring bias drift and feature attribution drift Amazon SageMaker Clarify](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_model_monitor/fairness_and_explainability/SageMaker-Model-Monitor-Fairness-and-Explainability.ipynb): Use Amazon SageMaker Model Monitor to monitor bias drift and feature attribution drift over time\.

These notebooks have been verified to run in Amazon SageMaker Studio only\. If you need instructions on how to open a notebook in Amazon SageMaker Studio, see [Create or Open an Amazon SageMaker Studio Notebook](notebooks-create-open.md)\. Select the **Python 3 \(Data Science\)** kernal if prompted to choose one\.

## Guide to the Amazon SageMaker Clarify documentation<a name="clarify-fairness-and-explainability-toc"></a>

Bias can occur and be measured in the data at each stage of the machine learning lifecycle: before training a model and after model training\. Feature attribution explanations of model predictions can provided for trained models and for models deployed to production, where they can be monitored for any drift from their baseline explanatory attributions\. The documentation for Amazon SageMaker Clarify is embedding throughout the larger SageMaker documentation set at their relevant ML stages\. Links to the high\-level topics are as follows\.
+ For further information on detecting bias in pre\-processing data before it used to train a model, see [Detect pre\-training data bias](clarify-detect-data-bias.md)
+ For further information on detecting post\-training data and model bias, see [Detect post\-training data and model bias](clarify-detect-post-training-bias.md)
+ For further information on the model\-agnostic feature attribution approach to explain model predictions after training, see [Model explainability](clarify-model-explainability.md)
+ For further information on monitoring for bias in production model inferences due to the drift of data away from the baseline used to train the model, see [Monitor bias drift for models in production](clarify-model-monitor-bias-drift.md)
+ For further information on monitoring for bias in production model inferences due to the drift of features contributions away from the baseline that was established during model training, see [Monitor feature attribution drift for models in production](clarify-model-monitor-feature-attribution-drift.md)
# Monitor bias drift for models in production<a name="clarify-model-monitor-bias-drift"></a>

Amazon SageMaker Clarify bias monitoring enables data scientists and machine learning \(ML\) engineers to monitor predictions for bias on a regular basis\. As the model is monitored, they can view exportable reports and graphs detailing bias in SageMaker Studio and configure alerts in Amazon CloudWatch to receive notifications if bias is detected\. One way bias can be introduced in deployed ML models is when the data used in training differs from the data used to generate predictions during production\. This is especially pronounced if the data used for training changes over time\. For example, fluctuating mortgage rates would result in less accurate model predictions unless the model is retrained and updated regularly\. A model for predicting home prices would become biased if the mortgage rates used to train the model differ significantly from the most current real\-world mortgage rates\. With bias detection capabilities in Model Monitor, SageMaker Clarify detects a changes in a model’s bias and it automatically generates metrics that can be viewed in SageMaker Studio and through Amazon CloudWatch alerts\. 

In general, measuring bias only during the train\-and\-deploy phase may not be sufficient\. It is quite possible that after the model has been deployed, the distribution of the data that the deployed model sees, the live data, is different from that of the training dataset\. This change may cause the model to exhibit more bias than it did on the training data\. The change in the live data distribution might be temporary, due to some short\-lived behavior like the holiday season, or permanent\. In either case, it might be important to detect these changes and take steps to reduce the bias when appropriate\.

To detect these changes, SageMaker Clarify provides functionality to monitor the bias metrics of a deployed model continuously and raise automated alerts if the metrics exceed a threshold\. To be concrete, take the DPPL bias metric as an example\. Let us say that you specify an allowed range of values A=\(amin​,amax​\), for instance an interval of \(\-0\.1, 0\.1\), that DPPL should belong to during deployment\. Any deviation from this range should raise a “bias detected” alert\. SageMaker Clarify allows you to perform these checks at regular intervals\.

For instance, you can set the frequency of the checks to be 2 days\. This means that every 2 days, SageMaker Clarify computes the DPPL on a Dwin​ data set that the model processed during last 2 days time window\. We would like an alert to be issued if the DPPL value bwin​ computed on Dwin​ falls outside of the allowed range A\. But this approach to checking if bwin​ is outside of A can be somewhat noise\-prone\. Dwin​ might consist of very few samples and might not be representative of the live data distribution\. The small sample size means that the value of bias bwin​ computed over Dwin​ may not be a very robust estimate\. In fact, very high \(or low\) values of bwin​ may be observed purely due to chance\. To ensure that the conclusions drawn from the observed data Dwin​ are statistically significant, SageMaker Clarify makes use of confidence intervals\. Specifically, it uses the Normal Bootstrap Interval method to construct an interval C=\(cmin​,cmax​\) such that we are confident that the true bias value computed over the full live data is contained in C with high probability\. Now, if our confidence interval C overlaps with the allowed range A, we interpret it as “it is likely that the bias metric value of the live data distribution falls within our allowed range”\. If C and A are disjoint, are confident that the bias metric does not lie in A and raise an alert\.

## Model Monitor sample notebook<a name="clarify-model-monitor-sample-notebooks-bias-drift"></a>

Amazon SageMaker Clarify provides the following sample notebook that shows how to capture real\-time inference data, create a baseline to monitor evolving bias against, and inspect the results\. The following topics contain the highlights from the last two steps\.
+ [Monitoring bias drift and feature attribution drift Amazon SageMaker Clarify](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_model_monitor/fairness_and_explainability/SageMaker-Model-Monitor-Fairness-and-Explainability.ipynb): Use Amazon SageMaker Model Monitor to monitor bias drift and feature attribution drift over time\.

This notebook has been verified to run in Amazon SageMaker Studio only\. If you need instructions on how to open a notebook in Amazon SageMaker Studio, see [Create or Open an Amazon SageMaker Studio Notebook](notebooks-create-open.md)\. Select the **Python 3 \(Data Science\)** kernal if prompted to choose one\. The following code samples are taken from the example notebook linked\. 

**Topics**
+ [Model Monitor sample notebook](#clarify-model-monitor-sample-notebooks-bias-drift)
+ [Create a bias drift baseline](clarify-model-monitor-bias-drift-baseline.md)
+ [Schedule feature attribute drift monitoring jobs](clarify-model-monitor-bias-drift-schedule.md)
+ [Inspect reports for data bias drift](clarify-model-monitor-bias-drift-report.md)